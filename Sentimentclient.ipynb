{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Staures92/Projet-de-fin-Data-Afique-Hub/blob/main/Sentimentclient.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jGMrdfXFrl_a",
        "outputId": "62e7e7aa-5fc8-4838-952f-09f231d517db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "rXGEkBrok5Tj",
        "outputId": "1fcb1dd8-e75a-458e-8205-f92d7313a4e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: streamlit in /usr/local/lib/python3.10/dist-packages (1.37.1)\n",
            "Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.2.2)\n",
            "Requirement already satisfied: blinker<2,>=1.0.0 in /usr/lib/python3/dist-packages (from streamlit) (1.4)\n",
            "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (5.4.0)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (8.1.7)\n",
            "Requirement already satisfied: numpy<3,>=1.20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (1.26.4)\n",
            "Requirement already satisfied: packaging<25,>=20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (24.1)\n",
            "Requirement already satisfied: pandas<3,>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (2.1.4)\n",
            "Requirement already satisfied: pillow<11,>=7.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (9.4.0)\n",
            "Requirement already satisfied: protobuf<6,>=3.20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (3.20.3)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (14.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.10/dist-packages (from streamlit) (2.32.3)\n",
            "Requirement already satisfied: rich<14,>=10.14.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (13.7.1)\n",
            "Requirement already satisfied: tenacity<9,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (8.5.0)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.10/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.12.2)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.10/dist-packages (from streamlit) (3.1.43)\n",
            "Requirement already satisfied: pydeck<1,>=0.8.0b4 in /usr/local/lib/python3.10/dist-packages (from streamlit) (0.9.1)\n",
            "Requirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.10/dist-packages (from streamlit) (6.3.3)\n",
            "Requirement already satisfied: watchdog<5,>=2.1.5 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.0.2)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (0.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (3.1.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (4.23.0)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (0.12.1)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.11)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.3.0->streamlit) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.3.0->streamlit) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.3.0->streamlit) (2024.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (2024.7.4)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit) (2.16.1)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (2.1.5)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (24.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.20.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.3.0->streamlit) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install streamlit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "9rUX9XftkN-A"
      },
      "outputs": [],
      "source": [
        "# Imoprtations des bibliothèques necessaires\n",
        "\n",
        "import pandas as pd # pandas pour charger et explorer les données\n",
        "import numpy as np\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import re\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Bidirectional, LSTM, Dense\n",
        "from keras.optimizers import Adam\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.pipeline import Pipeline\n",
        "import streamlit as st"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HYJfht23lIQh"
      },
      "source": [
        "# **Importation des données**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "yRn4iSHuxP8U"
      },
      "outputs": [],
      "source": [
        "#Charger le fichier d'entraînement\n",
        "df_train = pd.read_parquet('/content/drive/My Drive/train-00000-of-00001.parquet')\n",
        "\n",
        "# Charger le fichier de test\n",
        "df_test = pd.read_parquet('/content/drive/My Drive/test-00000-of-00001.parquet')\n",
        "\n",
        "# df_train = pd.read_parquet('train-00000-of-00001.parquet')\n",
        "# df_test = pd.read_parquet('test-00000-of-00001.parquet')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dyFpf7fNlVIq"
      },
      "source": [
        "# **Exploration des données**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aopyPX33lQnY",
        "outputId": "bd125619-cc70-4d1e-e862-a8131f2e9e84"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train data:\n",
            "   label                                               text\n",
            "0      4  dr. goldberg offers everything i look for in a...\n",
            "1      1  Unfortunately, the frustration of being Dr. Go...\n",
            "2      3  Been going to Dr. Goldberg for over 10 years. ...\n",
            "3      3  Got a letter in the mail last week that said D...\n",
            "4      0  I don't know what Dr. Goldberg was like before...\n",
            "5      4  Top notch doctor in a top notch practice. Can'...\n",
            "6      4  Dr. Eric Goldberg is a fantastic doctor who ha...\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 650000 entries, 0 to 649999\n",
            "Data columns (total 2 columns):\n",
            " #   Column  Non-Null Count   Dtype \n",
            "---  ------  --------------   ----- \n",
            " 0   label   650000 non-null  int64 \n",
            " 1   text    650000 non-null  object\n",
            "dtypes: int64(1), object(1)\n",
            "memory usage: 9.9+ MB\n",
            "None\n",
            "\n",
            "Test data:\n",
            "   label                                               text\n",
            "0      0  I got 'new' tires from them and within two wee...\n",
            "1      0  Don't waste your time.  We had two different p...\n",
            "2      0  All I can say is the worst! We were the only 2...\n",
            "3      0  I have been to this restaurant twice and was d...\n",
            "4      0  Food was NOT GOOD at all! My husband & I ate h...\n",
            "5      2  This is a tiny Starbucks and it locations like...\n",
            "6      1  Typical Starbucks coffee chain. 2 things I don...\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 50000 entries, 0 to 49999\n",
            "Data columns (total 2 columns):\n",
            " #   Column  Non-Null Count  Dtype \n",
            "---  ------  --------------  ----- \n",
            " 0   label   50000 non-null  int64 \n",
            " 1   text    50000 non-null  object\n",
            "dtypes: int64(1), object(1)\n",
            "memory usage: 781.4+ KB\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "print(\"Train data:\")\n",
        "print(df_train.head(7))\n",
        "print(df_train.info())\n",
        "\n",
        "print(\"\\nTest data:\")\n",
        "print(df_test.head(7))\n",
        "print(df_test.info())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LjFOnNYjlfX2"
      },
      "source": [
        "**Observation** : Les données fournies présentent une structure standard avec deux colonnes, \"label\" et \"text\", et une répartition entre un ensemble d'entraînement (650 000 entrées) et un ensemble de test (50 000 entrées).\n",
        "La colonne \"label\" est de type \"int64\", indiquant que les sentiments sont représentés par des nombres entiers.\n",
        "La colonne \"text\" est de type \"object\", ce qui est typique pour stocker des chaînes de caractères dans pandas.\n",
        "\n",
        "Cependant, les labels (étiquettes) indiquent une classification à 5 (de 0  à 4)classes, et les exemples de texte sont en anglais."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T_tFc5-flrvO"
      },
      "source": [
        "# **Prétraitement des données**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zf1sMdw9lhI2",
        "outputId": "a418d2ea-1548-43d5-f765-e206cd0a9b6a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "stop_words = set(stopwords.words('english')) # utiliser les stop words anglais puisque les exemples de texte sont en anglais"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "eDLqsM4Ml4EB"
      },
      "outputs": [],
      "source": [
        " # Le prétraitement inclut la suppression de la ponctuation, URLs,\n",
        " # hashtags, mentions d'utlisateurs et  la conversion en minuscules,\n",
        " # la tokenization et la suppression des stop words\n",
        "\n",
        " # definir une fonction de prétraitement de texte\n",
        "def preprocess_text(text):\n",
        "    # Supprimer les caractères spéciaux\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "\n",
        "    # Supprimer les URLs\n",
        "    text = re.sub(r'http\\S+', '', text)\n",
        "\n",
        "    # Supprimer les hashtags (mots commençant par #)\n",
        "    text = re.sub(r'#\\w+', '', text)\n",
        "\n",
        "    # Supprimer les mentions d'utilisateurs (mots commençant par @)\n",
        "    text = re.sub(r'@\\w+', '', text)\n",
        "\n",
        "    # Convertir en minuscules\n",
        "    text = text.lower()\n",
        "\n",
        "    # Tokenization\n",
        "    # Exemple d'utilisation de la fonction word_tokenize\n",
        "    # supposons que text = \"Check out this awesome #Python tutorial by @katwa! #programming https://www.example.com\"\n",
        "    # tokens va retourner un vecteur de mot individuel ['check', 'out', 'this', 'awesome', 'python', 'tutorial', 'by', 'katwa', 'programming']\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # Supprimer les stop words (mots vides) du texte tokenisé. Exemple des mots vide dans le text ('out', 'this', 'by')\n",
        "    # qui sont souvent supprimés lors du traitement de texte pour analyser le contenu principal\n",
        "    tokens = [word for word in tokens if word not in stop_words] # Resultat: ['check', 'awesome', 'python', 'tutorial', 'katwa', 'programming']\n",
        "    return ' '.join(tokens) # Retourne une chaîne unique en joignant tous les mots traités avec un espace (check awesome python tutorial katwa programming)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "8GjyzwVel_f-"
      },
      "outputs": [],
      "source": [
        "# Appliquer la fonction de prétraitement 'preprocess_text' à les colonnes'text' des deux DataFrames 'df_train' et 'df_test' et\n",
        "# stocker le résultat dans une nouvelle colonne appelée 'processed_text'\n",
        "\n",
        "df_train['processed_text'] = df_train['text'].apply(preprocess_text)\n",
        "df_test['processed_text'] = df_test['text'].apply(preprocess_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TYUSI6DwyBNf",
        "outputId": "e6bf188a-4ace-4491-9fdf-56520634cf03"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(   label                                               text  \\\n",
              " 0      4  dr. goldberg offers everything i look for in a...   \n",
              " 1      1  Unfortunately, the frustration of being Dr. Go...   \n",
              " 2      3  Been going to Dr. Goldberg for over 10 years. ...   \n",
              " \n",
              "                                       processed_text  \n",
              " 0  dr goldberg offers everything look general pra...  \n",
              " 1  unfortunately frustration dr goldbergs patient...  \n",
              " 2  going dr goldberg 10 years think one 1st patie...  ,\n",
              "    label                                               text  \\\n",
              " 0      0  I got 'new' tires from them and within two wee...   \n",
              " 1      0  Don't waste your time.  We had two different p...   \n",
              " 2      0  All I can say is the worst! We were the only 2...   \n",
              " \n",
              "                                       processed_text  \n",
              " 0  got new tires within two weeks got flat took c...  \n",
              " 1  dont waste time two different people come hous...  \n",
              " 2  say worst 2 people place lunch place freezing ...  )"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "df_train.head(3), df_test.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WDmo8938otAb"
      },
      "source": [
        "# **Entraînement et evaluations des modèles de classification des sentiments**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UeinjnRatwIU"
      },
      "source": [
        "***1. Modèle RNN (LSTM)***\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nFwFIKmIBH30"
      },
      "source": [
        "**Préparation des données pour l'entraînement du modèle**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize tokenizer\n",
        "#tokenizer = Tokenizer()\n",
        "#tokenizer.fit_on_texts(df_train['processed_text'])\n"
      ],
      "metadata": {
        "id": "3rDQrHnw2otG"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert text data to sequences\n",
        "#X_train_sequences = tokenizer.texts_to_sequences(df_train['processed_text'])\n",
        "#X_test_sequences = tokenizer.texts_to_sequences(df_test['processed_text'])"
      ],
      "metadata": {
        "id": "wC2gWD0x7lWO"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Pad sequences to a fixed length\n",
        "# The padding step ensures that all sequences have the same length, which is crucial for training deep learning models like LSTM\n",
        "#max_length = 100\n",
        "#X_train_padded = pad_sequences(X_train_sequences, maxlen=max_length, padding='post')\n",
        "#X_test_padded = pad_sequences(X_test_sequences, maxlen=max_length, padding='post')\n",
        "\n",
        "\n",
        "\n",
        "# Define the target variable\n",
        "y_train = df_train['label'].values\n",
        "y_test = df_test['label'].values"
      ],
      "metadata": {
        "id": "mmhVxexa6LLP"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "X_train = df_train['processed_text'].values  # Convert to numpy array\n",
        "X_test = df_test['processed_text'].values\n",
        "\n",
        "# Ensure X_train & X_test are in a 1D array of strings\n",
        "X_train= X_train.astype(str)\n",
        "X_test = X_test.astype(str)"
      ],
      "metadata": {
        "id": "bdN7MAL8xJO-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a pipeline with TF-IDF vectorizer and Logistic Regression\n",
        "pipeline = Pipeline([\n",
        "    ('tfidf', TfidfVectorizer(max_features=10000)),\n",
        "    ('clf', LogisticRegression(C=1.0, max_iter=1000, n_jobs=-1))\n",
        "])\n",
        "\n"
      ],
      "metadata": {
        "id": "80w6fOpvs4XZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "pipeline.fit(X_train, y_train)\n",
        "\n"
      ],
      "metadata": {
        "id": "YcGowHZcs4vD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions on the test set\n",
        "y_pred = pipeline.predict(X_test)\n",
        "\n",
        "# Print the classification report\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "QhKIBMqSs4_g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the model\n",
        "import joblib\n",
        "joblib.dump(pipeline, 'client_review_classifier.joblib')"
      ],
      "metadata": {
        "id": "DKPdAw6LvctH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the vocabulary size based on the tokenizer's word index\n",
        "# vocab_size = total number of unique words or tokens in the tokenized dataset\n",
        "\n",
        "vocab_size = len(tokenizer.word_index) + 1  # Adding 1 for the padding token"
      ],
      "metadata": {
        "id": "WR2nBqOL8g6q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the embedding dimension\n",
        "\n",
        "# The embedding dimension is a hyperparameter that determines the size of the vector representations of the words in the embedding layer\n",
        "# the embedding dimension value is often chosen to be between 100 to 300 for medium to large datasets.\n",
        "# Since we have a considerable number of samples in the training data, we can opt for a larger embedding dimension\n",
        "# to capture more intricate relationships between words in the vocabulary\n",
        "\n",
        "embedding_dim = 150  # In essence, this represents the dimensionality of the embedding space\n"
      ],
      "metadata": {
        "id": "hN-Ua9519f2X"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMohe9NMd524xodzr60vcKa",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}